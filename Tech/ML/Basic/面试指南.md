# 算法岗位面经
- http://gitlinux.net/2019-05-10-interview-questions/
- https://zhuanlan.zhihu.com/p/58434325
- GBDT https://www.cnblogs.com/ModifyRong/p/7744987.html
- https://blog.csdn.net/qq_24753293/article/details/80568104
- https://blog.csdn.net/akenseren/article/details/78787900
- http://jacoxu.com/%e8%ae%a1%e7%ae%97%e6%9c%bait%e6%b1%82%e8%81%8c%e5%ad%a6%e4%b9%a0list/

# SVM
- https://zhuanlan.zhihu.com/p/93715996
- https://zhuanlan.zhihu.com/p/31886934
- 台大林轩田https://www.cnblogs.com/inchbyinch/p/12556058.html
- SMO算法
	- https://www.cnblogs.com/biyeymyhjob/archive/2012/07/17/2591592.html
	- https://www.cnblogs.com/jerrylead/archive/2011/03/18/1988419.html
- SVM解决软分类问题https://blog.csdn.net/rikichou/article/details/78491491
- SVR林轩田讲解https://blog.csdn.net/malele4th/article/details/79181637

### 基础
- 几何间隔和函数间隔
- SVM的SV，SVM的公式推导（硬间隔和软间隔），KKT条件
	- https://blog.csdn.net/v_JULY_v/article/details/7624837
- 对偶问题（SVM是凸二次规划问题，可以直接用QP公式直接求解）
	- https://blog.csdn.net/qq_39422642/article/details/78755903
- SMO优化
	- https://zhuanlan.zhihu.com/p/32152421
	- 简化版SMO算法
		- 遍历查找第一个乘子找违背KKT条件的
		- 第二个乘子随机选择
	- 完整版SMO算法
		- 一种方式是在所有数据集上进行单遍扫描，另一种方式则是在非边界乘子（0!=a || a!=C）中实现单遍扫描
		- 通过最大化步长的方式来获取第二个乘子
- 用二次规划QP解二次优化问题，台大林轩田视频http://www.mamicode.com/info-detail-895787.html

### 细节
- SVM原问题和对偶问题的关系
	- 原始问题>=对偶问题
	- 当满足slater条件，且原始问题是凸的，使得满足强对偶关系，则原始问题=对偶问题
	- https://segmentfault.com/a/1190000016328439
- KKT条件是什么
	- https://zhuanlan.zhihu.com/p/26514613
- 核函数（哪个地方引入、画图解释高维映射，高斯核可以升到多少维，如何选择核函数）
	- 一般用线性核和高斯核
	- 高维可以实现线性可分，但是有维度灾难，所以引入核函数，核函数只是用来计算映射到高维空间之后的内积的一种简便方法

>下面是吴恩达的见解：
如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM
如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel
如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况
	
- 引入拉格朗日的优化方法的原因（该问题在逻辑回归也存在）
	- 把不等式约束问题转为无约束问题
	- 把原问题的复杂度（样本维度）降低至对偶问题的复杂度（样本数量），可以加入核函数
	- https://www.zhihu.com/question/26526858
- SVM的损失函数，目标函数中C的作用
	- 软间隔和Hinge损失函数+正则化项
	- C越大，对误分类的惩罚增大，对违规数据越不容忍，模型拟合能力越强
- SVM的优缺点
	- 优点
		- 鲁棒性
		- 升维
		- 计算开销
	- 缺点
		- 不适合大规模数据集
		- 对缺失数据敏感
		- 对参数敏感
- SVM与LR最大区别
	- https://www.zhihu.com/question/26768865
	- LR对数据分布敏感，数据类别必须balanced
	- LR一般不用核函数，计算开销太大
	- LR使用全部数据求最优解，SVM只用SV
	- LR和SVM都对异常点敏感
	- svm 更多的属于非参数模型，而logistic regression 是参数模型
- 为什么要把原问题转换为对偶问题？
	- 因为原问题是凸二次规划问题，转换为对偶问题更加高效。为什么求解对偶问题更加高效？因为只用求解alpha系数，而alpha系数只有支持向量才非0，其他全部为0，alpha系数有多少个？样本点的个数
- 如何解决多分类问题、可以做回归吗，怎么做？
	- 多分类：one-VS-all,one-VS-one,层次SVM
	- 回归任务：SVR
- SVM和逻辑斯特回归对同一样本A进行训练，如果某类中增加一些数据点，那么原来的决策边界分别会怎么变化？
	- https://www.zhihu.com/question/30123068
- **各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归和最大熵模型**
	- https://www.zhihu.com/question/26726794

# LR
### 基础
- https://chenrudan.github.io/blog/2016/01/09/logisticregression.html
- LR推导，有没有最优解？
	- 如果一个函数是凸优化问题，仍然可能没有最优解。无约束的凸优化问题可能无下界（e.g.线性函数）或最优值有限但不可达，存在最优解的充要条件是存在一阶导数为零的点。逻辑斯特回归是凸优化问题，凸函数+无约束条件。有最优解是因为存在一阶导数为零的点。
	- https://blog.csdn.net/menc15/article/details/72231939
	- https://zhuanlan.zhihu.com/p/74874291
- 多分类LR https://www.cnblogs.com/lianyingteng/p/7784158.html


### 细节

- 基于LR的SVM软分类模型
	- https://blog.csdn.net/rikichou/article/details/78491491
	- https://blog.csdn.net/red_stone1/article/details/74506323
- 逻辑回归如何处理非线性可分任务？
	- 核逻辑回归
	- 多个逻辑回归模型组合（神经网络）
- LR模型为什么用指数？softmax为什么使用指数？
	- softmax使用指数函数，放大大的值，缩小小的值
	- LR使用主要是观察对数几率的线性模型
- 什么是对数几率函数？
- LR和SVM的异同
- 为什么不选平方损失函数呢？
	- 梯度更新的速度和sigmod函数本身的梯度是很相关的，训练速度慢
- 逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？为什么我们还是会在训练的过程当中将高度相关的特征去掉？
	- 先说结论，如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。
	- 但是对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一。
	- 如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。
	- 去掉高度相关的特征会让模型的可解释性更好
	- 可以大大提高训练的速度。如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。其次是特征多了，本身就会增大训练的时间。
- LR可以用核么？可以怎么用？l1和l2正则项是啥？lr加l1还是l2好？加哪个可以用核（加l2正则项，和svm类似，加l2正则项可以用核方便处理）
	- 线性回归的贝叶斯解释
	- https://www.jianshu.com/p/4d562f2c06b8
	- LR作为判别模型，同时是参数模型，逻辑回归是假设y服从Bernoulli分布
	- L1:为模型参数添加一个拉普拉斯分布先验
	- L2:为模型参数添加一个均值为0的正太分布先验
	- 进行kernel trick时，需要存储a参数和训练数据x，计算量随着样本数据的增加而增加
	- **LR和FM的关系**
- LR可以用来处理非线性问题么？（还是lr啊 只不过是加了核的lr 这里加核是显式地把特征映射到高维 然后再做lr）怎么做？可以像SVM那样么？为什么？
	- 可以引入交叉项等作为显式的高维映射，类似特征组合，但是需要根据业务设定，数据稀疏导致部分参数训练不够，可能影响模型性能
	- 使用FM模型，对于因子分解机FM来说，最大的特点是对于稀疏的数据具有很好的学习能力。
- 为什么LR需要归一化或者取对数，为什么LR把特征离散化后效果更好？
	- 归一化或者取对数可以去量纲，有利于梯度下降
	- LR把特征离散化后，可以增强数据的表达能力，可以实现类似特征组合的功能，并且对离群点鲁棒，模型更稳定，稀疏矩阵计算快
- LR 处理多分类任务
	- Softmax回归及其损失函数
	- one-VS-all
- 参数模型和非参数模型的定义
	- 参数模型通常假设总体（随机变量）服从某一个分布，该分布由一些参数确定（比如正太分布由均值和方差确定），在此基础上构建的模型称为参数模型；
非参数模型对于总体的分布不做任何假设，只是知道总体是一个随机变量，其分布是存在的（分布中也可能存在参数），但是无法知道其分布的形式，更不知道分布的相关参数，只有在给定一些样本的条件下，能够依据非参数统计的方法进行推断。
- LR的并行化
	- 按行和列分布式存储
	- label的取值范围是-1和+1，其损失函数从分类正确的角度设计
	- http://blog.sina.com.cn/s/blog_6cb8e53d0101oetv.html
- LR和Bayes的统一性
	- 特征由连续值和onehot组成，在Ck的条件下服从正态分布N(u,E)，注意E跟Ck无关，证明该结论，并分析不符合的情况下如何解决
	- https://www.zhihu.com/question/23652394
	- GNB与LR对比总结http://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf
- 逻辑回归的值表示概率吗？（值越大可能性越高，但不能说是概率）
	- https://www.jianshu.com/p/a8d6b40da0cf?utm_campaign=haruki&utm_content=note&utm_medium=reader_share&utm_source=weixin
	- 从指数族分布角度解释（伯努利分布假设和线性假设）
	- softmax 回归也需要满足一定的假设才能当做概率（需要满足多项式分布假设和线性假设）
- *看没看过scikit-learn源码LR的实现？（调用的liblinear）*
- 手推逻辑回归目标函数，正类是1，反类是-1。一般都是正例是1，反例是0的
	- 从极大似然估计出发，仅仅改变了指数部分
- naive bayes和logistic regression的区别
	- https://www.zhihu.com/question/265995680
	- 朴素贝叶斯里有一个很强的假设，就是条件独立假设，假设特征之间都是相互独立的，没有耦合的，互不干扰的。因此，朴素贝叶斯可以不使用梯度下降，而直接通过统计每个特征的逻辑发生比来当做权重
	- 逻辑回归，条件独立假设并不成立，通过梯度下降法，可以得到特征之间的耦合信息，从而得到相应的权重。
	- 如果GNB假设成立，在渐进意义上，NB和LR会收敛成同一个分类器，否则，二者一般会训练出不同的分类器，这时，当数据量无限时，LR的结果往往比NB要好
	- 但是GNB（高斯朴素贝叶斯，假设连续变量服从正态）与LR相比，GNB会快速的收敛，并且需要log n（n是数据维度）级别的样本数据。而LR需要n级别的数据，这时，对于小数据量下，GNB可能会比LR好一些（生成模型一般需求的训练数据比判别模型少）
	- 生成模型和判别模型的区别
- LR为什么用sigmoid函数。这个函数有什么优点和缺点？为什么不用其他函数？
	- 逻辑回归在 0 附近敏感，在远离 0 点位置不敏感，模型更加关注分类边界，可以增加模型的鲁棒性
- LR和最大熵模型http://www.win-vector.com/dfiles/LogisticRegressionMaxEnt.pdf
- LR的牛顿法https://www.cs.mcgill.ca/~dprecup/courses/ML/Lectures/ml-lecture05.pdf

# L1、L2和拟合问题
https://www.cnblogs.com/nxf-rabbit75/p/9954394.html
### 基础
- L0,L1,L2的定义，Lp范数的定义
	- L0范数：向量中非0元素的个数
	- L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择
	- L2正则化可以防止模型过拟合；一定程度上，L1也可以防止过拟合
- 过拟合的原因
	- 模型过度复杂，训练数据不足，在有限的样本中搜索过大的模型空间
	- 训练过程中模型过多吸收了噪声数据的影响
	- 训练集和测试机特征分布不一致

### 细节
- Lp范数最小化进行数据拟合时的意义
	- https://blog.csdn.net/chenisok/article/details/80358330
	- L0范数为众数回归
	- L1范数为中位数回归https://www.zhihu.com/question/46664595
	- L2范数为平均数回归
- L2为什么可以使得参数衰减
	- 从最小二乘法、贝叶斯、主成分分析以及偏差-方差理论分析L2正则化可以使参数衰减https://zhuanlan.zhihu.com/p/32488420
- 深度学习里面怎么防止过拟合？（data aug；dropout；multi-task learning）
	- 4种主要防止过拟合方法：Early Stopping、数据集扩充、正则化法以及dropout
	- 在输入数据、权值和网络的前向传播部分加入噪声Noise
- 机器学习中使用「正则化来防止过拟合」到底是一个什么原理？为什么正则化项就可以防止过拟合？
	- bias-variance decomposition
	- 为过大的模型空间加一个限制，在有限的数据中选择泛化能力强的模型
	- 贝叶斯解释，给参数W一个先验分布
	- **PACLearning理论与VC维**
		- Learning Theory与VC维http://xtf615.com/2017/03/29/Learning-Theory/
- 多重共线性问题
	- 那么共线性的问题是不是必须被解决呢？那也不一定，取决于我们的目的。如果我们的目的是判断哪些变量对于我们的预测有显著的影响的话，那么显然共线性的存在会影响我们的决策和判断。而如果我们只是想要得到一个比较准确的预测值，即给一些变量X去获得一个预测值，那么不存在共线性并不是必须的。
	- 计量经济学中级教程https://blog.csdn.net/diyiziran/article/details/17025471
	- 判别方法
		- 方差膨胀因子VIF
		- 相关性分析，相关系数高于0.8，表明存在多重共线性；但相关系数低，并不能表示不存在多重共线性
		- 条件系数检验；
	- 解决方法
		- PCA方法解决
		- 增加样本容量
		- 逐步回归
		- 删除一部分共线性变量
		- L2可以减少共线性的问题，使得XTX可逆，得唯一解，但是加入惩罚项得出的参数是有偏的 
			- L1可以解决多重共线性问题吗？为什么https://blog.csdn.net/bbbeoy/article/details/72526164
			- https://www.jianshu.com/p/ef1b27b8aee0
			- https://posts.careerengine.us/p/5b0b6224ec908a73fe30faf8
	- 回归分析的五个基本假设
	- 仅仅存在于回归算法
	
- 为什么加入了正则项就能够避免过拟合？
	- 加入正则项和的参数估计是符合我们之前的预定目标的，即用尽量少的变量去拟合数据。压缩了模型空间，奥卡姆剃刀原则
- 惩罚力度alpha太大了容易拟合不足，太低了容易过度拟合。究竟多大的惩罚力度是合适的？
	- AIC和BIC，选最低点
	- https://www.zhihu.com/question/38121173/answer/104955389
- 最小二乘原理、L1和L2 与 极大似然估计 的关系
	- https://blog.csdn.net/To_be_to_thought/article/details/81511096
- 如何确定发生了过拟合？什么是过拟合？
	- 模型在训练集和测试集上表现不一致就是过拟合的最主要的现象
	- 过拟合是一种现象，目前缺乏理论定义
	- 自助法产生的数据集改变了原始数据集的分布，虽然在数据集较小、难以有效划分训练/测试集时，可以产生1/3的测试集检测过拟合，但会引入额外的估计偏差
- L1范数，L2范数的异同点分别是什么，各自用在什么地方？如何解决 L1 求导困难
	- L1减少的是一个常量，L2减少的是权重的固定比例
	- L1使权重稀疏，L2使权重衰减，一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0
	- L1求极值的方法：
		- 近端梯度下降https://www.zhihu.com/question/38426074
			- 公式注解https://www.jianshu.com/p/0987a8841573
			- https://zhuanlan.zhihu.com/p/82622940
			- https://zhuanlan.zhihu.com/p/86824796
			- 软阈值函数证明https://blog.csdn.net/bit_666/article/details/80051737
		- 次梯度下降法 (subgradient descent) 
			- https://blog.csdn.net/bitcarmanlee/article/details/51896348
			- https://zhuanlan.zhihu.com/p/36784881
		- 坐标轴下降法（coordinate descent）
		- 最小角回归法（ Least Angle Regression，LARS）https://www.cnblogs.com/pinard/p/6018889.html
	- 一般用L2，因为计算方便，用 L2 一定只有一条最好的预测线，L1 则因为其性质可能存在多个最优解。L1鲁棒性 (Robust) 更强，对异常值更不敏感（因其减少量是一个常量）。https://www.zhihu.com/question/26485586
		- L1 则因为其性质可能存在多个最优解https://www.bradthiessen.com/html5/docs/ols.pdf
- 为什么L2正则化可以获得值很小的参数？请通过参数迭代公式解释，推导L1和L2的迭代公式
	- https://www.cnblogs.com/tsruixi/p/10693120.html
	- https://zhuanlan.zhihu.com/p/88250866
- 为什么L1和L2的解空间是不同的？
	- 带正则项等价于带约束条件，通过KKT条件解释（《百面机器学习》）
- MAE与MSE分别有什么性质？

# 树模型
### 基础
- ID3、C4.5、CART树及其剪枝
	- C4.5的连续值属性：根据增益率公式，我们可以发现，当分界点能够把样本分成数量相等的两个子集时（我们称此时的分界点为等分分界点），增益率的抑制会被最大化，因此等分分界点被过分抑制了。子集样本个数能够影响分界点，显然不合理。因此在决定分界点是还是采用增益这个指标，而选择属性的时候才使用增益率这个指标。这个改进能够很好得抑制连续值属性的倾向。
		- http://blog.sina.com.cn/s/blog_68ffc7a40100urn3.html
	- C4.5离散值属性：因为信息增益比偏向取值较少的特征，所有C4.5并不是直接选择信息增益率最大的特征，而是先在候选特征中找出信息增益高于平均水平的特征，然后在这些特征中再选择信息增益率最高的特征
		- https://www.cnblogs.com/muzixi/p/6566803.html
	- 树剪枝
		- C4.5剪枝方案
			- 错误率降低剪枝和悲观剪枝https://www.cnblogs.com/zhangchaoyang/articles/2842490.html
		- https://github.com/appleyuchi/Decision_Tree_Prune
		- https://blog.csdn.net/appleyuchi/article/details/83692381
	- CART树剪枝
		- https://www.zhihu.com/question/22697086/answer/821565832
		- https://online.stat.psu.edu/stat508/lesson/11/11.8/11.8.2
		- https://www.jianshu.com/p/b90a9ce05b28
		- https://www.youtube.com/watch?v=D0efHEJsfHo
		- https://www.deeplearn.me/1714.html
		- https://www.cnblogs.com/pinard/p/6053344.html
		- 《统计学习方法》里有个错误https://www.pianshen.com/article/4803892828/
	- CART树剪枝流程Cost-Complexity Pruning(CCP)

```
1. 使用全量样本生成一个完全树T_0, alpha=0
2. 增加alpha值，计算树分值（C(T_0)+alpha*|T_leaves|），直到对T_0的内部节点进行剪枝会有更小的树分值时，进行剪枝，生成树T_1
3. 增加alpha值，计算树分值（C(T_1)+alpha*|T_leaves|），直到对T_1的内部节点进行剪枝会有更小的树分值时，进行剪枝，生成树T_2
4. 重复步骤3，直到T_n只有一个叶节点时停止循环
5. 将全量数据分成训练数据和测试数据，并用训练数据和之前寻找的alpha序列生成子树序列
6. 用测试数据在子树序列中寻找损失（分类任务时Gini指数，回归任务时均方误差）最小的子树
7. 返回步骤5，如此循环，直到完成10-fold cross validation
8. 对于alpha序列中的每个值，平均10-fold cross validation中的误差，误差值最小的alpha是最终alpha，并从最初使用全量数据生成的子树序列中，选择该alpha生成的子树，作为最终结果
```
- CART树处理缺失值（这个在数据量很大的情况下开销太大，而带来的性能提升确很有限，所以后来就不怎么用这种处理方式）
	- surrogate splits：https://zhuanlan.zhihu.com/p/86679767
- boosting：Adaboost、GBDT、Xgboost的推导和理论依据
	- 分类问题和回归问题
- bagging：RF的实现和理论依据

|  | 树结构 | 分裂标准|建树偏好 |处理离散特征 | 处理连续特征 | 处理缺失值 | 分类任务 | 回归任务 | 剪枝 |   
|---|-----|--------|--------|-------|------|------|----|-----------------|------|
| ID3| 多叉  |信息增益|优先选择可取值数目较多的属性构建分支 | √      | ×      | ×     | √    | ×    | ×  |  
| C4\.5| 多叉  |增益率|离散特征：优先选择可取值数目较少的特征构建分支；连续特征：当分界点能够把样本分成数量相等的两个子集时，增益率的抑制会被最大化| √      | √      | √     | √    | ×    | √  | 
| CART| 二叉  |Gini系数| 无               |√      | √      | √     | √    | √    | √  | 

### 细节
- 3种决策树的区别和优缺点对比
	- https://www.jianshu.com/p/6bbee9fd79d0
	- https://www.cnblogs.com/pinard/p/6050306.html
	- https://www.cnblogs.com/pinard/p/6053344.html
- CART做回归任务的损失函数
- CART树为什么用Gini系数衡量数据的不纯度或者不确定性
	- 熵的对数运算比Gini的二次运算计算量大
	- https://www.cnblogs.com/pinard/p/6050306.html
	- https://www.cnblogs.com/pinard/p/6053344.html
	- 信息熵和基尼指数的关系(信息熵在x=1处一阶泰勒展开就是基尼指数)，熵到Gini系数的推导https://blog.csdn.net/lanchunhui/article/details/65441891
- CART树缺失值处理
	- 使用surrogate splits方法
		- 该方法计算量大，但是效果一般，所以很少使用
		- https://www.salford-systems.com/resources/webinars-tutorials/tips-and-tricks/using-surrogates-to-improve-datasets-with-missing-values
		- https://stats.stackexchange.com/questions/171574/meaning-of-surrogate-split
- C4.5决策树连续值处理方法和缺失值处理方法；
	- 缺失值三问
		- 训练数据中有样本数据属性值缺失，应如何选择最优属性？
		- 含缺失值样本如何分入子节点
		- 测试样本属性缺失时，如何输出结果
		- https://blog.csdn.net/leaf_zizi/article/details/83503167
	- 连续值处理方法
		- 分箱https://www.cnblogs.com/keye/p/10564914.html

===
		

- RF，GBDT的区别；GBDT，Xgboost的区别
- 很深的树和GBDT可以在训练集上有一样好的表现，为什么使用GBDT，而不用决策树
	- 单棵树的泛化能力不足，比如异或关系就无法学习
- SVM、LR、决策树的对比？
	- https://www.jianshu.com/p/743cf2357b28
	- 线性SVM依赖数据表达的距离测度，所以需要对数据先做normalization，LR不受其影响，但是LR使用梯度下降方法，所以一般需要normalization归一化https://www.zhihu.com/question/26768865/answer/34078149
- RF中树的生成过程需要剪枝吗？为什么
- LR、SVM和决策树，分别是参数模型还是非参数模型
	- https://blog.csdn.net/taoqick/article/details/102644413
- 解释下随机森林和gbdt的区别。gbdt的boosting体现在哪里。解释下随机森林节点的分裂策略，以及它和gbdt做分类有什么区别？哪个效果更好些？为什么？哪个更容易过拟合？为什么？ 随机森林和lr的优缺点对比， adaboost和随机森林的比较，为了防止随机森林过拟合可以怎么做
	- 模型都会过拟合，即使RF拥有足够多的树，随机森林的过拟合问题https://blog.csdn.net/u010429286/article/details/100101768
- 随机森林的基分类器有哪些？可否由决策树替换为线性分类器或者KNN？
- 随机森林需要交叉验证吗？
	- https://zhuanlan.zhihu.com/p/77473961
- gbdt做二分类时，初值F0(x)该如何设置，树节点根据什么分裂，叶节点的输出值如何确定，预测时如何计算输出，做多分类呢？gbdt做回归时，根据什么分裂，叶节点的输出值如何确定？
	- https://zhuanlan.zhihu.com/p/46445201
	- https://blog.csdn.net/On_theway10/article/details/83576715
	- 分类方法实现https://blog.csdn.net/qq_22238533/article/details/79185969，叶节点的输出值计算
	- 分类任务使用交叉熵损失函数，基模型是回归树，分支节点使用varience gain查找最优分割点，叶节点输出对数几率值
- gbdt怎么并发（特征层面，树层面不能并发）
- 如何使用GBDT做特征组合和RF做特征选择？为什么信息增益可以用来选特征？
	-  RF做特征选择https://blog.csdn.net/Forlogen/article/details/88078217
	-  GBDT做特征组合https://blog.csdn.net/olizxq/article/details/89061924
- xgboost相对原始gbdt做了哪些改动，有什么特点？lightGBM呢？
	- xgboost
		- 牛顿法
		- 正则化：叶节点个数和叶节点分数之和
		- 缺失值处理方式
		- 最优特征值搜索：近似搜索
		- 使用损失函数作为指标查找最优分裂点
		- 行抽样列抽样
		- level-wise生成树，再剪枝
	- lightGBM
		- GOSS
		- EFB
		- leaf-wise生成树
- sparkML中GBDT和RF源码分析
- GBDT是否需要特征归一化
- lgb和xgboost论文https://blog.csdn.net/songbinxu/article/details/79452547#22_GBDT_237
	- lgb解答https://yifdu.github.io/2019/03/17/%E5%A4%A7%E6%9D%80%E5%99%A8Lightgbm/

# 集成学习
- 集成学习的定义
	- https://toutiao.io/posts/y5vljj/preview
	- https://blog.csdn.net/liuweiyuxiang/article/details/83023565
- stacking的实现步骤
	- https://zhuanlan.zhihu.com/p/40693176
- 常见融合框架原理，优缺点，bagging，stacking，boosting
- 为什么融合能提升效果
	- 把模型看作是从不同角度看待数据，模型融合旨在结合多个角度共同分析数据
- 为什么boosting不容易过拟合？
	- https://www.zhihu.com/question/41047671
- *是否了解线性加权、bagging、boosting、cascade等模型融合方式*
- 如果使用模型融合Stacking效果没有提升反而下降，原因是什么？
	- https://blog.csdn.net/Li_yi_chao/article/details/89638009
- stacking、boosting和bagging的异同点
	- 追求目标不同
	- 基分类器要求不同
	- uniform方式不同
- 集成学习的两个关键点
	- https://github.com/THUDataPI/TechnologyOverview/blob/master/%E4%B8%80%E6%96%87%E8%AF%BB%E6%87%82%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/%E4%B8%80%E6%96%87%E8%AF%BB%E6%87%82%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0.md
	- 如何构建具有差异性的分类器
	- 多这些分类器的结果进行整合


# 朴素贝叶斯
- 朴素贝叶斯的理解
	- https://www.cnblogs.com/pinard/p/6069267.html
- 朴素贝叶斯分类器原理以及公式，出现估计概率值为0怎么处理（拉普拉斯平滑）；
	- 拉普拉斯修正https://blog.csdn.net/xo3ylAF9kGs/article/details/78636400
- 贝叶斯分类，这是一类分类方法，主要代表是朴素贝叶斯，朴素贝叶斯的原理，重点在假设各个属性类条件独立。然后能根据贝叶斯公式具体推导。如何利用朴素贝叶斯分类去分类，比如：给你一个人的特征，判断是男是女，比如身高，体重，头发长度等特征的的数据，那么你要能推导这个过程。给出最后的分类器公式。
- 那你说说贝叶斯怎么分类啊？比如说看看今天天气怎么样？
	- 利用天气的历史数据，可以知道天气类型的先验分布，以及每种类型下特征数据（比如天气数据的特征：温度啊，湿度啊）的条件分布，这样我们根据贝叶斯公式就能求得天气类型的后验分布了
	- 下溢出问题使用对数解决https://blog.csdn.net/DILIGENT203/article/details/83934809
- *半朴素贝叶斯模型*
	- https://blog.csdn.net/xo3ylAF9kGs/article/details/78643424

# k-means和GMM
### 基础
- k-means 聚类的原理；kmeans算法的优缺点及对应的改进
	- kmeans变体
		- https://www.biaodianfu.com/k-means.html
		- https://blog.csdn.net/u014465639/article/details/71342072
	- 缺点
		- 局部最优
		- k值选择，二分K-Means
		- 初始中心点选择，k-means++
		- 样本间距离计算 kernel k-means，Elkan K-Means
		- 对异常敏感，可改成求点的中位数，这种聚类方式即K-Mediods聚类
		- 对于不是凸的数据集比较难收敛
		- 不能发现非凸形状的簇，或大小差别很大的簇
		- 数据量大时，计算开销大，mini-batch k-means
		- 时间复杂度高O(nkt)，其中n是对象总数，k是簇数，t是迭代次数。
		- 只适用于数值型数据，只能发现球型类簇
- KMeans的算法伪代码
	- https://hpu-yz.github.io/2019/07/19/K-means%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8Apython%E5%AE%9E%E7%8E%B0/
- 什么是EM算法
	- EM的推导https://www.pianshen.com/article/704290903/
	- 当模型中有隐变量时，求解隐变量对应参数最优解
	- 缺点：对初值敏感，容易陷入局部最优
- GMM定义和推导

### 细节
- k值的选择
	- https://www.biaodianfu.com/k-means-choose-k.html
	- 拍脑袋法，将样本量除以2再平方根出来的值作为K值
	- 肘部法则（Elbow Method）
	- 间隔统计量 Gap Statistic
	- 轮廓系数（Silhouette Coefficient）
	- Canopy算法，通过事先粗聚类的方式，为k-means算法确定初始聚类中心个数和聚类中心点
- Kmeans 算法 K 怎么设置、适用什么样数据集、怎么评价 Kmeans 聚类结果、 Kmeans 有什么优缺点？你的项目中使用 Kmeans 遇到哪些问题，怎么解决的?
- EM的收敛性证明
	- 极大似然估计单调增加
	- 当似然估计不再增加或变化幅度很小，则EM收敛
	- https://www.cnblogs.com/jerrylead/archive/2011/04/06/2006936.html
- 采用 EM 算法求解的模型有哪些，为什么不用牛顿法或梯度下降法？
	- 带有隐变量，往往可以用EM算法来求解
	- 用牛顿法等求导困难，所以选择EM
	- EM算法类似坐标上升法https://blog.csdn.net/google19890102/article/details/51065297
- EM的优缺点
	- 最优解和初值有关
	- 容易陷入局部最优
- EM和梯度下降的关系
	- https://kexue.fm/archives/4277
- EM 与 kmeans 的关系，用 EM 算法推导解释 Kmeans
	- k-means中每个样本所属的类就可以看成是一个隐变量，在E步中，我们固定每个类的中心，通过对每一个样本选择最近的类优化目标函数，在M步，重新更新每个类的中心点，该步骤可以通过对目标函数求导实现，最终可得新的类中心就是类中样本的均值。
	- https://www.zhihu.com/question/49972233
- 如何判断自己实现的 LR、Kmeans 算法是否正确？
- 如何用spark实现k-means
	- https://blog.csdn.net/goodstuddayupyyeah/article/details/75020659
- kmeans和GMM的关系
	- k-means算法是高斯混合聚类在混合成分方差相等，且每个样本仅指派一个混合成分时候的特例。
	- https://www.cnblogs.com/arachis/p/KMeans.html
	- https://www.zhihu.com/question/31296149

# 聚类
- 用过哪些聚类算法，解释密度聚类算法
	- k-means和knn算法
	- https://www.jiqizhixin.com/articles/the-6-clustering-algorithms-data-scientists-need-to-know
- **聚类算法中的距离度量有哪些**
	- 余弦距离，曼哈顿距离，欧式距离
	- https://www.cnblogs.com/daniel-D/p/3244718.html
	- https://blog.csdn.net/jbfsdzpp/article/details/48497347
- **度量聚类的指标**
	- 轮廓系数
	- https://zhuanlan.zhihu.com/p/78920991
	- https://blog.csdn.net/qq_28935065/article/details/82811413
	- https://www.jianshu.com/p/611ecd46bd35

# PCA、SVD和LDA（Linear Discriminant Analysis）
https://blog.csdn.net/sunmenggmail/article/details/8071502
### 基础
- PCA和SVD定义及其推导
	- PCA的两种推导思想，https://blog.csdn.net/yyhhlancelot/article/details/90680288
	- 线性代数基础https://zhuanlan.zhihu.com/p/55297233
- LDA的定义及其推导，二类和多类
	- 类内聚合，类间分离，降维到c-1维（c为类别数量）

### 细节
- SVD和聚类的关系
	- https://kexue.fm/archives/4216
- PCA和SVD的关系，他们的使用场景和优缺点
	- PCA和SVD比较分析https://zhuanlan.zhihu.com/p/58064462
- LDA的作用和优缺点
- *NFM介绍*，https://blog.csdn.net/sunmenggmail/article/details/8071502

# HMM
- https://www.cnblogs.com/pinard/p/6945257.html
- https://blog.csdn.net/qq_27586341/article/details/94602772

### 基础
- HMM的定义
- 一个模型，两个假设，三个问题
	- 两个假设
		- 齐次马尔可夫假设
		- 观测独立
	- 三个问题
		- evaluation（前向-后向算法）
		- learning（Baum-Welch）
		- decoding（Viterbi）

### 细节
- 实现 hmm 的状态转移代码（动态规划）
- 前向-后向算法和viterbi算法的时间复杂度
	- 	https://www.cnblogs.com/sjjsxl/p/6285629.html
- 为什么Baum-Welch是EM的特例
	- Baum-Welch算法就是从EM算法演变出来的
	- https://www.bilibili.com/video/BV1MW41167Rf?p=5
	- 什么是Jenson不等式https://blog.csdn.net/baidu_38172402/article/details/89090383
	- https://blog.csdn.net/firparks/article/details/54934112
- HMM，MEMM和CRF的关系，他们的异同点
- 贝叶斯网络和马尔可夫随机场的异同


# 机器学习理论
- 什么是最小二乘法
	- https://blog.csdn.net/qll125596718/article/details/8248249
- https://blog.csdn.net/akenseren/article/details/78787900
- 讲机器学习中常用的损失函数有哪些？交叉熵有什么好处？（凸优化问题）
	- 交叉熵在机器学习中的应用https://www.zhihu.com/question/65288314/answer/244557337
	- 交叉熵处理分类问题比MSE收敛更快
	- 在逻辑回归二次代价函数存在很多局部最小点，而交叉熵就不会https://www.cnblogs.com/hugh2006/p/11691369.html
- 判别模型与生成模型的本质区别是什么
- 分类模型和回归模型的区别，分类模型可以做回归分析吗？反过来可以吗？
	- 可以https://www.zhihu.com/question/21329754
- 为什么要用交叉验证，k折交叉验证中k取值多少有什么关系 
	- https://zhuanlan.zhihu.com/p/31924220
- 解释局部相关性
	- https://blog.csdn.net/weixin_42193719/article/details/103980333
- **在模型的训练迭代中，怎么评估效果**；
- 特征选择方法有哪些(能说出来10种以上加分)，之后和面试官仔细聊了一下特征选择的问题，我介绍了了解的几种基本的特征选择思路（错误率选择、基于熵的选择、类内类间距离的选择）；
	- 介绍filter，https://zhuanlan.zhihu.com/p/99073860 
	- 概述https://www.cnblogs.com/yuesi/articles/9236796.html
	- 介绍filter、wrapper和embedded，https://www.cnblogs.com/bjwu/p/9103002.html
- 维度灾难与过拟合的异同点
	- https://www.jianshu.com/p/867193608bbd
- 推导softmax损失函数
	- https://www.cnblogs.com/zongfa/p/8971213.html
- 如何用尽可能少的样本训练模型同时又保证模型的性能
- 线性分类和非线性分类各有哪些模型
- 比较各个模型的Loss function
	- 感知机的损失函数https://www.jianshu.com/p/c91087e6e1ea
- 绝对值损失函数的最优解是变量的中位数，证明
	- https://zhuanlan.zhihu.com/p/35944715
	- http://web.uvic.ca/~dgiles/blog/median2.pdf
	- https://www.le.ac.uk/users/dsgp1/COURSES/THIRDMET/MYLECTURES/1REGRESS.pdf
- 设计一个结构存取稀疏矩阵（面试官最后告诉我了一个极度压缩的存法，相同行或列存偏差）
- *PageRank原理*，怎么用模型来查找异常用户,为什么不用规则
- 无监督和有监督算法的区别
- 如何处理数据中的噪声点、数据清洗算法
	- https://www.zhihu.com/question/22077960
	- 缺失值如何填充https://zhuanlan.zhihu.com/p/59853229
- 是否了解A/B Test以及A/B Test结果的置信度特征工程经验，是否了解mutual infomation、**chi-square**、*LR前后向*、树模型等特征选择方式
- 深刻理解各种算法对应采用的数据结构和对应的搜索方法。比如KNN对应的KD树、如何给图结构设计数据结构？如何将算法map-red化
	- knn与kd树，kd树要求数据维度不能过大https://blog.csdn.net/suibianshen2012/article/details/51234596
- 矩阵的各种变换，尤其是特征值相关的知识。分布式的矩阵向量乘的算法
- 线性分类器与非线性分类器的区别及优劣；特征比数据量还大时，选择什么样的分类器？对于维度很高的特征，你是选择线性还是非线性分类器？对于维度极低的特征，你是选择线性还是非线性分类器？如何解决过拟合问题？L1和L2正则的区别，如何选择L1和L2正则？
- 项目中的数据是否会归一化处理，哪个机器学习算法不需要归一化处理

# 深度学习
### 基础
- 深度学习概述https://www.zhihu.com/question/34681168
- 深度学习基础http://www.deeplearning.net/tutorial/
- DNN、CNN和RNN定义及其特点，他们之间的区别
	- 局部连接可以保证获取局部信息；权值共享保证高效
	- cnn特性
		- 同样的pattern可能在图片不同的位置出现
		- 图片中的一些pattern比整体图片小
		- 池化层：subsampling，丢弃部分像素后，图像依然可辨
		- 更少的参数w
		- 共享权重（每个con2D层的一个filter有多少参数）
- 画RNN的图，LSTM图
	- LSTM图解https://www.bilibili.com/video/BV15b411g7Wd?p=43
	- https://www.bilibili.com/video/BV1Qb411p7mG
- CNN实现过程介绍
	- https://www.bilibili.com/video/BV15b411g7Wd?p=20
- DNN参数更新公式的推导

### 细节
- 模型的初始化
	- LSTM：每个神经元中：cell中隐状态的初始化和4个权重matrix的初始化
	- cnn：所有filter的初始值
- 宽网络和深网络，优缺点对比
- 激活函数有哪些，优缺点对比
	- sigmoid，tanh，relu
- 深度网络初值设定的要求
	- 不能全部相同
- 如何用BPTT训练RNN，如何用BP训练cnn
	- cnn训练https://blog.csdn.net/m0_37490039/article/details/79378143
	- BPTT公式http://ir.hit.edu.cn/~jguo/docs/notes/bptt.pdf
	- BPTT推导https://blog.csdn.net/sysstc/article/details/75333008?depth_1-utm_source=distribute.pc_relevant.none-task&utm_source=distribute.pc_relevant.none-task
	- RNN推导http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.16.6652&rep=rep1&type=pdf
- RNN的激活函数为什么一定用sigmoid而不用relu
	- https://www.zhihu.com/question/61265076
- 梯度消失和梯度爆炸的原因，如何解决
	- rnn的梯度爆炸和梯度消失是因为一个RNN单元使用时间序列数据训练使用的是同一个W，随着时间，不断连乘导致的
		- LSTM使用加法而不是乘法，所以不会有vanishing的问题
		- LSTM可以解决梯度消失，但不解决梯度爆炸，所以学习率设的尽可能小
		- https://www.bilibili.com/video/BV15b411g7Wd?p=44
	- 激活函数
	- LSTM可以解决梯度消失，clipping解决梯度爆炸
- 神经网络有哪些层，各层的特点是什么
- rnn模型的num_units参数怎么解释
	- https://blog.csdn.net/notHeadache/article/details/81164264
	- https://mp.weixin.qq.com/s/D8HNQZ1dGzXhxZWP2ElXdw
	- https://zhuanlan.zhihu.com/p/28919765
- 为什么CNN要用权值共享？（每个卷积核相当于一个特征提取器，它的任务是匹配局部图像中的特征，权值共享后，匹配的特征方式都是一样的，提取若干特征后就知道学习的是啥了）
- 如果出现过拟合该怎么办
	- Batch Normalization：https://zhuanlan.zhihu.com/p/34879333
		- https://blog.csdn.net/Fate_fjh/article/details/53375881?depth_1-utm_source=distribute.pc_relevant.none-task&utm_source=distribute.pc_relevant.none-task
	- Dropout：
		- https://blog.csdn.net/stdcoutzyx/article/details/49022443
		- https://zhuanlan.zhihu.com/p/38200980
	- L1和L2

- 如果本身training loss就很大你怎么办？如果数据不变，怎么调整网络结构解决这个问题
	- https://blog.ailemon.me/2019/02/26/solution-to-loss-doesnt-drop-in-nn-train/
- BN层，池化层，dropout层是什么，功能是什么，都怎么用
- 当全连接跟dropout连着用需要注意什么
	- 训练时要放缩数据乘以1/(1-p)，否则测试阶段就要放缩乘以p
- 神经网络用来防止过拟合的方法https://www.zhihu.com/question/59201590
	- 在集成学习中，为什么从N个模型中随机选择一个模型的期望误差，要比所有模型输出的平均误差大
	- 为什么加入噪声可以防止过拟合

- dnn和rnn中的梯度消失原理一样么？dnn中是哪个部分导致梯度消失？（激活层如sigmoid，cnn和dnn可以使用relu）rnn中怎么解决梯度消失问题？（lstm的结构相对普通RNN多了加和，为避免梯度消散提供了可能。线性自连接的memory是关键。）
- CNN里面有哪些层？讲一下卷积。卷积的形式是啥样？给定一个输入，算输出的feature map大小。卷积有啥用？池化有啥用？有哪些池化方式？池化除了降采样还有啥用
	- 池化层有：max，L2，mean
	- 池化除了降采样还有啥用
		- 可以一定程度提高空间不变性
		- pooling可以减少模型参数https://www.zhihu.com/question/40727704
- dropout内部是怎么实现只让部分信号通过并不更新其余部分对于输入的权值的？讲讲BN（BatchNormalization）为什么好？
- RNN里的梯度消失一般怎么处理？细讲下lstm的结构，这样设计为什么好？（门关闭，当前信息不需要，只有历史依赖；门打开，历史和当前加权平均）你觉得梯度消失靠引入一些新的激活层可以完全解决么？为什么？


# 优化
https://www.julyedu.com/question/big/kp_id/23/ques_id/1524
https://www.cnblogs.com/shixiangwan/p/7532830.html
### 基础
- 梯度下降，牛顿法，拟牛顿法
	- 牛顿法、阻尼牛顿法、拟牛顿法https://blog.csdn.net/itplus/article/details/21896453
- https://www.cnblogs.com/shixiangwan/p/7532830.html

### 细节
- 梯度下降的优缺点；主要问最优化方面的知识，梯度下降法的原理以及各个变种（批量梯度下降，随机梯度下降法， mini-batch梯度下降法），以及这几个方法会不会有局部最优问题
	- https://blog.csdn.net/xiaotao_1/article/details/81031633
- 牛顿法原理和适用场景（损失函数二阶可导），有什么缺点，如何改进（拟牛顿法），会不会有局部最优问题，牛顿法的二次收敛
- 如果有若干个极小值点，如何避免陷入局部最优解

- 如何判断函数凸或非凸？
	- https://www.zhihu.com/question/49902644
	- https://blog.csdn.net/xmu_jupiter/article/details/47400411
	- 如果Hessian矩阵是半正定矩阵，则是f(X)凸函数
- *最速下降法和共轭梯度法 wolfe条件 最速下降法和共轭梯度法的收敛速度如何判断*
- *深刻理解常用的优化方法：梯度下降、牛顿法、各种随机搜索算法（基因、蚁群等等），深刻理解的意思是你要知道梯度下降是用平面来逼近局部，牛顿法是用曲面逼近局部等等*
- 神经网络的优化器有哪些
	- 深度学习
		- adaptive learning rate
			- adagrad：用历史一次导数的RMS去近似一个二次导数https://www.bilibili.com/video/BV15b411g7Wd?p=2
			- RMSprop：adagrad的根号内进行指数加权平均
			- https://www.bilibili.com/video/BV1JE411g7XF?p=12
		- 改变GD的方向
			- Momentum：优化方向由当前梯度方向和上一个梯度方向共同决定
			- adam：RMSProp + Momentum
		- stochastic gradient descent
		- mini-batch
- 激活函数
	- ReLU
		- leaky ReLU
		- Maxout：learnable activation，和dropout类似，不用的w不会被训练

		
# SparkML
